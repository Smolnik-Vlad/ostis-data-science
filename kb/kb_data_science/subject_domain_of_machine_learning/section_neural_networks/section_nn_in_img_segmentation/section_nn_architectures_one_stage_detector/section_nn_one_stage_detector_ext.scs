concept_yolo

	=> nrel_main_idtf: 
		["YOLO"] 
			(* <-lang_ru;; *);
	=> nrel_main_idtf: 
		["YOLO"] 
			(* <-lang_en;; *);;
			
concept_yolo

	<- rrel_key_sc_element:
		...
		(*
		
			<- sc_definition;;
				=>nrel_main_idtf: [Определение: YOLO](*<-lang_ru;;*);;
				=>nrel_main_idtf: [Definition: YOLO](*<-lang_en;;*);;
				
			<=nrel_sc_text_translation: ...
				(*
					->rrel_example:
					[Что такое YOLO? Эта аббревиатура расшифровывается как “You Only Look Once” (“Стоит только раз взглянуть”). YOLO — современный алгоритм глубокого обучения, который широко используется для обнаружения объектов. Он был разработан Джозефом Редмоном и Али Фархади в 2016 году. Основное отличие YOLO от других алгоритмов сверточной нейронной сети (CNN), используемых для обнаружения объектов, заключается в том, что он очень быстро опознает объекты в режиме реального времени. Принцип работы YOLO подразумевает ввод сразу всего изображения, которое проходит через сверточную нейронную сеть только один раз. Именно поэтому он называется “Стоит только раз взглянуть”. В других алгоритмах этот процесс происходит многократно, то есть изображение проходит через CNN снова и снова. Так что YOLO обладает преимуществом высокоскоростного обнаружения объектов, чем не могут похвастать другие алгоритмы.](*<-lang_ru;;*);;
					->rrel_example:
					[YOLO (“You Only Look Once”) is an effective real-time object recognition algorithm, first described in the seminal 2015 paper by Joseph Redmon et al. Not long after, YOLO dominated the object-detection field and became the most popular algorithm used, because of its speed, accuracy, and learning ability.

Instead of treating object detection as a classification problem, the authors thought about it as a regression task concerning spatially separated bounding boxes and associated class probabilities, using a single neural network. The YOLOv1 processed images in real-time at 45 frames per second, while a smaller version – Fast YOLO – reached 155 frames per second and still achieved double the mAP of other real-time detectors.](*<-lang_en;;*);;
					
				*);;
		*);;


concept_ssd

	=> nrel_main_idtf: 
		["SSD"] 
			(* <-lang_ru;; *);
	=> nrel_main_idtf: 
		["SSD"] 
			(* <-lang_en;; *);;
			
concept_ssd

	<- rrel_key_sc_element:
		...
		(*
		
			<- sc_definition;;
				=>nrel_main_idtf: [Определение: SSD](*<-lang_ru;;*);;
				=>nrel_main_idtf: [Definition: SSD](*<-lang_en;;*);;
				
			<=nrel_sc_text_translation: ...
				(*
					->rrel_example:
					[Single Shot Detector - это метод сегментации изображений, принадлежащий к семейству Region Free детекторов. Single Shot Detector (SSD) выполняет один проход изображения для выявления нескольких объектов. SSD рассматривается как одна из самых быстрых моделей определения объектов с довольно высокой точностью.

SSD состоит из двух компонентов: основная модель и головная часть SSD. Основная модель - это предварительно обученная сеть классификации изображений, работающая как экстрактор объектов. Обычно это сеть, такая как ResNet, обученная по Imagenet, из которой удален финальный, полностью соединенный слой классификации. Головная часть SSD - это один или несколько сверточных слоев, добавленных к основной модели, выходные данные интерпретируются как ограничивающие прямоугольники и классы объектов в пространственном местоположении активаций финального слоя.](*<-lang_ru;;*);;
					->rrel_example:
					[Single Shot Detectors (SSDs) are a popular and efficient type of method used to perform object segmentation tasks. SSD has two components: a backbone model and SSD head. Backbone model usually is a pre-trained image classification network as a feature extractor. This is typically a network like ResNet trained on ImageNet from which the final fully connected classification layer has been removed. We are thus left with a deep neural network that is able to extract semantic meaning from the input image while preserving the spatial structure of the image albeit at a lower resolution. For ResNet34, the backbone results in a 256 7x7 feature maps for an input image. We will explain what feature and feature map are later on. The SSD head is just one or more convolutional layers added to this backbone and the outputs are interpreted as the bounding boxes and classes of objects in the spatial location of the final layers activations.](*<-lang_en;;*);;
					
				*);;
		*);;
		